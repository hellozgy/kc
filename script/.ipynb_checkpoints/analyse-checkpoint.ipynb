{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计词频\n",
    "def statis(src, words):\n",
    "    with open(src, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            for word in line.lower().split():\n",
    "                words[word] = words.get(word, 0) + 1\n",
    "    return words\n",
    "words = dict()\n",
    "words = statis('../input/train_data_bpe.csv', words)\n",
    "words = statis('../input/test_data_bpe.csv', words)\n",
    "with open('../input/word_freq_bpe.csv', 'w', encoding='utf-8') as fw:\n",
    "    for k,v in sorted(words.items(), key=lambda x:(x[1], len(x[0])), reverse=True):\n",
    "        fw.write('{} {}\\n'.format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               toxic   severe_toxic        obscene         threat  \\\n",
      "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
      "mean        0.095844       0.009996       0.052948       0.002996   \n",
      "std         0.294379       0.099477       0.223931       0.054650   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "              insult  identity_hate  \n",
      "count  159571.000000  159571.000000  \n",
      "mean        0.049364       0.008805  \n",
      "std         0.216627       0.093420  \n",
      "min         0.000000       0.000000  \n",
      "25%         0.000000       0.000000  \n",
      "50%         0.000000       0.000000  \n",
      "75%         0.000000       0.000000  \n",
      "max         1.000000       1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../input/train.csv')\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312405\n",
      "4.91891615051\n",
      "3.7140414666\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x160e79b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re,os\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "# 统计句子数分布\n",
    "os.system('rm ../input/long.csv')\n",
    "def statis(src, sents):\n",
    "    with open(src, 'r', encoding='utf-8') as f, open('../input/long.csv', 'a', encoding='utf-8') as fw:\n",
    "        for num, line in enumerate(f):\n",
    "            leng = len(re.split('[,.?!]', line))\n",
    "            if leng<=15:\n",
    "                sents.append(leng)\n",
    "            else:\n",
    "                leng = len(re.split('[.?!]', line))\n",
    "                if leng<=15:\n",
    "                    sents.append(leng)\n",
    "                else:\n",
    "                    leng = len(re.split('[?!]', line))\n",
    "                    if leng<=15:\n",
    "                        sents.append(leng)\n",
    "                    else:\n",
    "                        fw.write(line.strip()+'\\n')\n",
    "    return sents\n",
    "sents = []\n",
    "sents = statis('../input/train_data_bpe.csv', sents)\n",
    "sents = statis('../input/test_data_bpe.csv', sents)\n",
    "s = np.asarray(sents)\n",
    "print(len(s))\n",
    "print(np.mean(s))\n",
    "print(np.std(s))\n",
    "pl.hist(s)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看数据\n",
    "train_data = '../input/train_data.csv'\n",
    "train_label = '../input/train_label.csv'\n",
    "def f(fname):\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        res = []\n",
    "        for line in f:\n",
    "            res.append(line.strip())\n",
    "        return res\n",
    "datas = f(train_data)\n",
    "labels = f(train_label)\n",
    "labels = [t.split(',')[1:] for t in labels]\n",
    "\n",
    "tmp = '../input/tmp.csv'\n",
    "def ff(datas, labels, yes, no, tofile):\n",
    "    with open(tmp, 'w', encoding='utf-8') as fw:\n",
    "        for d, label in zip(datas, labels): \n",
    "            if label[yes]=='1' and label[no]=='0':\n",
    "                fw.write(d+'\\n')\n",
    "ff(datas, labels, 5, 0, tmp)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 得到脏字表\n",
    "def f(fname):\n",
    "    res = []\n",
    "    with open(fname, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "              res.append(line.strip().lower())  \n",
    "    return res\n",
    "base_dir = '../outdata/dict/'\n",
    "res = set()\n",
    "t = f(base_dir + 'badwords.txt')\n",
    "for tt in t:\n",
    "    for ss in tt.split(','):\n",
    "        res.add(ss.strip())\n",
    "t = f(base_dir + 'full-list-of-bad-words-banned-by-google-txt-file_2013_11_26_04_53_31_867.txt')\n",
    "for tt in t:\n",
    "    res.add(tt.replace('牋?', ''))\n",
    "t = f(base_dir + 'google_twunter_lol')\n",
    "res.update(t)\n",
    "t = f(base_dir + 'swearWords.txt')\n",
    "res.update(t)\n",
    "t = f(base_dir+'Terms-to-Block.csv')\n",
    "for tt in t:\n",
    "    res.add(tt.replace(',\"', ''))\n",
    "\n",
    "res2 = set()\n",
    "for t in res:\n",
    "    res2.add(t)\n",
    "    res2.add(t.replace(' ', ''))\n",
    "res = res2\n",
    "res = list(res)\n",
    "res.sort()\n",
    "\n",
    "with open('../input/badword.txt', 'w', encoding='utf-8') as fw:\n",
    "    for w in res:\n",
    "        fw.write(w+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# 处理外部数据\n",
    "tmp = '../input/tmp.csv'\n",
    "rate = (0.932335,0.174051,0.781389,0.038974, 1.0,0.147264)\n",
    "def fwrite(fname, data):\n",
    "    with open(fname, 'w', encoding='utf-8') as fw:\n",
    "        for line in data:\n",
    "            fw.write(line+'\\n')\n",
    "\n",
    "def solve1(data_files, label_files):\n",
    "    '''处理比赛举办方的数据'''\n",
    "    res = []\n",
    "    for dfile,lfile in zip(data_files, label_files):\n",
    "        labels = {}\n",
    "        with open(lfile, 'r', encoding='utf-8') as f:\n",
    "            first = True\n",
    "            for label in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                label = label.strip().split('\\t')\n",
    "                if label[0] in labels:labels[label[0]].append(float(label[-1]))\n",
    "                else:labels[label[0]] = [float(label[-1])]\n",
    "        with open(dfile, 'r', encoding='utf-8') as f:\n",
    "            first = True\n",
    "            for data in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                data = data.strip().split('\\t')\n",
    "                if data[0] in labels:\n",
    "                    d = data[1]\n",
    "                    d = d.replace('NEWLINE_TOKENNEWLINE_TOKEN', ' ').replace('NEWLINE_TOKEN',' ').strip()\n",
    "                    label = np.mean(labels[data[0]])\n",
    "                    label = ','.join([str(int(t*label*100)/100) for t in rate])\n",
    "                    res.append([label, d])\n",
    "    return res\n",
    "res = solve1(['../outdata/4054689/attack_annotated_comments.tsv', '../outdata/4563973/toxicity_annotated_comments.tsv'], \n",
    "        ['../outdata/4054689/attack_annotations.tsv','../outdata/4563973/toxicity_annotations.tsv'])\n",
    "\n",
    "# def scv1(path, label):\n",
    "#     res = []\n",
    "#     label = ','.join([str(int(t*label*100)/100) for t in rate])\n",
    "#     for file in os.listdir(path):\n",
    "#         with open(path+file, 'r', encoding='utf-8') as f:\n",
    "#             line = f.readline().strip()\n",
    "#             res.append([label, line])\n",
    "#     return res\n",
    "# t = scv1('../input/outdata/data-sarc-sample/notsarc/', 0)\n",
    "# res.extend(t)\n",
    "# t = scv1('../input/outdata/data-sarc-sample/sarc/', 1)\n",
    "# res.extend(t)\n",
    "\n",
    "'''kaggle insult'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275550\n",
      "284379\n",
      "204080\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv,re\n",
    "\n",
    "'''\n",
    "外部数据\n",
    "'''\n",
    "\n",
    "# 比赛主办方官方数据1\n",
    "datas=pd.read_csv('../outdata/4054689/attack_annotated_comments.tsv', sep = '\\t')\n",
    "datas = datas.loc[:,['rev_id','comment']]\n",
    "labels = pd.read_csv('../outdata/4054689/attack_annotations.tsv', sep = '\\t')\n",
    "labels = labels.loc[:,['rev_id','attack']]\n",
    "labels = labels.groupby(['rev_id']).mean().reset_index()\n",
    "mdatas = pd.merge(datas, labels, on=['rev_id'])\n",
    "mdatas.rename(columns={'attack':'insult'}, inplace = True)\n",
    "res = mdatas\n",
    "\n",
    "datas=pd.read_csv('../outdata/4563973/toxicity_annotated_comments.tsv', sep = '\\t')\n",
    "datas = datas.loc[:,['rev_id','comment']]\n",
    "labels = pd.read_csv('../outdata/4563973/toxicity_annotations.tsv', sep = '\\t')\n",
    "labels = labels.loc[:,['rev_id','toxicity']]\n",
    "labels = labels.groupby(['rev_id']).mean().reset_index()\n",
    "mdatas = pd.merge(datas, labels, on=['rev_id'])\n",
    "mdatas.rename(columns={'toxicity':'insult'}, inplace = True)\n",
    "res = pd.concat([res, mdatas])\n",
    "res['comment']=res['comment'].apply(lambda x:x.replace('NEWLINE_TOKEN',' ').replace('TAB_TOKEN', ' '))\n",
    "del res['rev_id']\n",
    "print(len(res))\n",
    "\n",
    "#kaggle insult\n",
    "datas=pd.read_csv('../outdata/kaglge insult/impermium_verification_labels.csv', sep = ',')\n",
    "datas = datas.loc[:,['Insult','Comment']]\n",
    "datas.rename(columns={'Insult':'insult', 'Comment':'comment'}, inplace = True)\n",
    "res = pd.concat([res, datas])\n",
    "\n",
    "datas=pd.read_csv('../outdata/kaglge insult/test_with_solutions.csv', sep = ',')\n",
    "datas = datas.loc[:,['Insult','Comment']]\n",
    "datas.rename(columns={'Insult':'insult', 'Comment':'comment'}, inplace = True)\n",
    "res = pd.concat([res, datas])\n",
    "\n",
    "datas=pd.read_csv('../outdata/kaglge insult/train.csv', sep = ',')\n",
    "datas = datas.loc[:,['Insult','Comment']]\n",
    "datas.rename(columns={'Insult':'insult', 'Comment':'comment'}, inplace = True)\n",
    "res = pd.concat([res, datas])\n",
    "res['comment']=res['comment'].apply(lambda x:x.replace(\"\\\\n\",\" \").replace(\"\\\\t\",\" \").replace(\"\\\\xa0\",\" \").replace(\"\\\\xc2\",\" \")\n",
    "                                   .replace('\\n', ' ').replace('\\\\x80', ' ').replace('\\\\xe2',' ').replace('\\xa0',' ').replace('\\ ',' ')\n",
    "                                   .replace('==',' ').strip().lower())\n",
    "res['comment']=res['comment'].apply(lambda x:re.sub('\\\\s+',' ',x))\n",
    "res['comment']=res['comment'].fillna('unknown', inplace=True)\n",
    "print(len(res))\n",
    "res = res.groupby(['comment']).mean().reset_index()\n",
    "print(len(res))\n",
    "rate = (0.932335,0.174051,0.781389,0.038974, 1.0,0.147264)\n",
    "res['insult'] = res['insult'].apply(lambda x:','.join([str(int(x*t*100)/100) for t in rate]))\n",
    "res['insult'].to_frame().to_csv('../input/outdata_label.csv', header=False, index=False, encoding='utf-8', quotechar=' ')\n",
    "res['comment'].to_frame().to_csv('../input/outdata_oneline.csv', header=False, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               toxic   severe_toxic        obscene         threat  \\\n",
      "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
      "mean        0.095844       0.009996       0.052948       0.002996   \n",
      "std         0.294379       0.099477       0.223931       0.054650   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "              insult  identity_hate  \n",
      "count  159571.000000  159571.000000  \n",
      "mean        0.049364       0.008805  \n",
      "std         0.216627       0.093420  \n",
      "min         0.000000       0.000000  \n",
      "25%         0.000000       0.000000  \n",
      "50%         0.000000       0.000000  \n",
      "75%         0.000000       0.000000  \n",
      "max         1.000000       1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../input/train.csv')\n",
    "#df=df.loc[df[\"insult\"] == 1]\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111542\n"
     ]
    }
   ],
   "source": [
    "train=set()\n",
    "with open('../input/train_data.csv','r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        train.add(line.strip())\n",
    "outdatas = []\n",
    "outdatas_bpe = []\n",
    "outdatas_labels = []\n",
    "with  open('../input/outdata_data.csv', 'r', encoding='utf-8') as datas,\\\n",
    "        open('../input/outdata_data_bpe.csv','r',encoding='utf-8') as datas_bpe,\\\n",
    "        open('../input/outdata_label.csv','r',encoding='utf-8') as labels:\n",
    "    for data,data_bpe,label in zip(datas, datas_bpe,labels):\n",
    "        data = data.strip()\n",
    "        data_bpe = data_bpe.strip()\n",
    "        label = lable.strip()\n",
    "        if data!='unknown' and data not in train:\n",
    "            outdatas.append(data)\n",
    "            outdatas_bpe.append(data_bpe)\n",
    "            outdatas_labels.append(label)\n",
    "with  open('../input/outdata_data2.csv', 'w', encoding='utf-8') as datas, \\\n",
    "        open('../input/outdata_data_bpe2.csv','w',encoding='utf-8') as datas_bpe, \\\n",
    "        open('../input/outdata_label2.csv','w',encoding='utf-8') as labels:\n",
    "    for data,data_bpe,label in zip(outdatas, outdatas_bpe, outdatas_labels):\n",
    "        datas.write(data+'\\n')\n",
    "        datas_bpe.write(data_bpe+'\\n')\n",
    "        labels.write(label+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
